---
title: 'Dirty work: getting your data ready for analysis'
author: "P. Roudier"
date: "26 June 2017"
output: 
  html_document:
    toc: true
    toc_float: true 
    theme: yeti
---

# Packages

```{r pkg, eval=FALSE}
pkg <- c(
  "sf",
  "tidyverse",
  "randomForest",
  "plyr",
  "raster",
  "mapview",
  "RColorBrewer",
  "dygraphs"
  "xts"
)

install.packages(pkg)
```

*** 
# From *messy* to *tidy* data

![](http://static.thinkr.fr/clean-your-data.jpg)

A very significant part of your time *will* be spent formatting data.

**Data tidying** is the operation of transforming data into a clear and simple form that makes it easy to work with:

- Very often one wants one row per observation/sample, with each column representing a measured/estimated variable.
- There is always a range of mistakes to fix in your raw data so to make automation/scripting possible

Data tidying is the most unglamourous, frustrating step on your way to Pedometrics glory. In this quick tutorial I will introduce some tools I frequently use to put my data into shape *before I start the analysis*.

## The tidyverse

In this tutorial I will showcase functions from a *family of packages* called the **tidyverse**. 

![*Hadley Whickam, 2017*](https://rviews.rstudio.com/post/2017-06-09-What-is-the-tidyverse_files/tidyverse1.png)

Some functionalities from the `tidyverse` are already present in `base` R, or provided by other packages, but this set of tools are following common logic, and usually make this data tidying step:

- *a little bit* less frustrating
- *a little bit* faster

### The `tidyverse` family of packages

One of the mantras of Hadley Whickam --- the creator of the `tidyverse` --- is inspired from the Unix philospophy:

> Build simple tools that do one thing --- and do it well.

This results into a rather extensive, and ever growing, collection of R packages that share a common philosophy, rather than one unique, monolithic package. 

These set of tools have been coded with the same philosophy in mind:

> 1. Reuse existing data structures.
> 2. Compose simple functions with the pipe.
> 3. Embrace functional programming.
> 4. Design for humans.

It is well worth checking out the website: `http://tidyverse.org/`. There is also a dedicated book which I assume is excellent (I haven't readit yet): *R for Data Science*.

### Meet your new friends!

> 2. Compose simple functions [with the pipe].

The key packages that implement the `tidyverse` philosophy or R programming:

```{r load_tidy, message=FALSE}
library(dplyr) # a grammar of data manipulation
library(tidyr) # play with data dimensions
library(magrittr) # Create "pipelines"
```

Some other packages are more specific towards a task or a type of data:

```{r load_tidy_2, message=FALSE}
library(readr) # rea fflat/text files (such as CSV)d
library(stringr) # manipulating character strings
library(forcats) # handle categorical data more easily
```

And more packages, not covered by this tutorial:

```{r load_tidy_3, message=FALSE, eval=FALSE}
library(lubridate) # manipulation of date and times data
library(purrr) # advanced functional programming
library(readxl) # read data directly from Microsoft Excel files
```

### Loading the `tidyverse` all at once

A handy tip: you can load the whole family of *tidy* packages by calling the `tidyverse` library:

```{r load_tidyverse, message=FALSE}
library(tidyverse)
```

## `data.frame` FTW `r emo::ji("victory")`

>  1. Reuse existing data structures.

At the centre of this set of tols is the `data.frame`. The `data.frame` is the most common data structure in R. Tools from the `tidyverse` all only work on `data.frame`. 

Some of these tools introduced a very similar data structure called `tibble`. It is very similar to the `data.frame` -- and actually it *is* a very specific `data.frame`:

```{r tibble}
tbl <- tibble(a = 1:3, b = c('a', 'b', 'c'))
tbl
is.data.frame(tbl)
```

You can convert very easily a `data.frame` to a `tibble`, and vice and versa:

```{r tibble_2}
head(mtcars) # print the first 5 lines of the mtcars dataset
class(mtcars)

tbl_mtcars <- as_tibble(mtcars)
class(tbl_mtcars)

as.data.frame(tbl_mtcars)
```

You can think of the `tibble` as water-downed version of the `data.frame`. It does *less* --- in particular it won't take any decisions for you (for example converting strings to factors), and is more focused towards what's actually needed during an interactive data analysis session:

- only prints the first few rows and columns
- prints the type of each column

## Pipelines

> 4. Design for humans.

There's a tool that fits very well in this picture: the *pipe* operator. 

The pipe operator is denoted `%>%`, and is introduced by the `magrittr` package. 

**To insert a pipe in RStudio**, type: `Ctrl + Shift + M`.

![](./pipe.jpeg)

What it does is to "pass" the result of a function to another function. Therefore:

```{r magrittr}
head(mtcars) # print the first 5 line sof the mtcars dataset

# Consider the following example:
mtcars %>%
  subset(hp > 100) %>%
  aggregate(. ~ cyl, data = ., FUN = . %>% mean %>% round(2)) %>%
  transform(kpl = mpg %>% multiply_by(0.4251)) %>%
  print

# Its horrific alternative:
transform(aggregate(. ~ cyl, data = subset(mtcars, hp > 100), FUN = function(x) round(mean(x, 2))), kpl = mpg*0.4251)

# Another equally problematic alternative:
res1 <- subset(mtcars, hp > 100)
res2 <- aggregate(. ~ cyl, data = res1, FUN = function(x) round(mean(x, 2)))
res3 <- transform(res2, kpl = mpg*0.4251)
```

There is *always* more than one fundamental operation to apply to any given dataset.

What `magrittr` offers is a simple solution to the *naming* or *obscufaction* problems. You don't have to use it, but it makes your code more readable.

## Now let's load some data

```{r load_aqp, message=FALSE}
# let's load some soil profile data
data(sp1, package  = 'aqp')
sp1 <- as_tibble(sp1)
```

### Strings

The `stringr` package provides a wide range of tools to handle *strings of characters*. These are often a headache when dealing with soil data (typos, rogue white spaces, etc).

Its functions are prefixed with `str_`:

```{r stringr}
# Detect a pattern
str_detect(sp1$name, 'A')

# Replace a pattern by something else
str_replace_all(sp1$name, '[0-9]', '')
str_replace_all(sp1$name, '[0-9]', '')

# These can be chained using the `%>%`  opeartor
sp1$name %>% 
  str_replace_all('[0-9]', '') %>% 
  str_replace_all('[a-z]', '')

str_extract_all(sp1$name, '[A-Z]', simplify = TRUE) %>% 
  head
```

## The tidyverse verbs

The `tidyverse` provides a set of basic verbs corresponding to each specific task of data manipulation:

- Filter specific rows: `filter`(and `slice`)
- Select specific variables: `select`
- Select unique rows: `distinct`
- Create new variables: `mutate` (and `transmute`)
- Arrange rows by variables: `arrange`
- Summarise multiple values into a single one: `summarise`
- Sample rows: `sample_n` (and `sample_frac`)

Remember *the zen of tidy data*:

> 4. Design for humans.

### Filter

Filtering specific rows according to their attributes:

```{r filter, message=FALSE}
sp1 %>% 
  filter(name == 'A1')

sp1 %>% 
  filter(name == "A1" | name == "A2" | name == "A3")

# Using stringr for string manipulation
sp1 %>% 
  filter(str_detect(name, 'A') & field_ph >8)

sp1 %>% 
  slice(1:3)
```

### Select

Selecting attributes according to their name or position.

```{r select}
sp1 %>% 
  select(1:3)

sp1 %>% 
  select(id, top, bottom, field_ph) 

sp1 %>% 
  select(id:bottom)

sp1 %>% 
  select(-(id:bottom))

sp1 %>% 
  select(starts_with('b'))
```

### Rename

Change the name of a variable.

```{r rename}
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  rename(ph = field_ph)
```

### Arrange

Sort the rows according to one (or several) variable.

```{r arrange}
# Sort horizon data according to field_ph
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  arrange(field_ph) 

# Sort according to field_ph THEN chroma
sp1 %>% 
  select(id, top, bottom, field_ph, chroma) %>% 
  arrange(field_ph, chroma) 

# Sort in descending order
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  arrange(desc(field_ph)) 
```

### Extract unique rows

Similar to `unique()` but much faster:

```{r distinct}
sp1 %>% 
  distinct(id)
```

### Add new columns

`mutate` (and `transmutate`) are very useful tools: they allow you to create a new variable (eg from existing ones). `transmute` drops existing variables. 

```{r mutate}
# `mutate` creates one (or more!) new variables
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  mutate(log_ph = log(field_ph))

# you can refer to variables you just created
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  mutate(
    log_ph = log(field_ph),
    log_ph_p1 = log_ph + 1
  )

# `transmute` drops existing variables
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  transmute(log_ph = log(field_ph))
```

### Summarise a group of rows into a unique value

Apply a function returning a **unique** value to a group of values. This covers a wide range of statistical indicators, such as the mean, the min/max, etc.

```{r summarise}
sp1 %>% 
  summarise(
    max_depth = max(bottom),
    mean_ph = mean(field_ph, na.rm = TRUE)
  )
```

### Random sampling of rows

The `sample_n` and `sample_frac` select a **random** number or fraction of rows from a dataset:

```{r sample}
sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  sample_n(3)

sp1 %>% 
  select(id, top, bottom, field_ph) %>% 
  sample_frac(0.2)
```

### Separate and group columns

The `unite` function can group two columns together to create a new, single variable. For example we might want to create a new variavle `pretty_depth` from the `top` and `bottom` attributes. It would look like `<TOP>--<BOTTOM> cm`:

```{r unite}
sp1 %>% 
  unite(pretty_depth, top, bottom, sep = '--')

# To add "cm" at the end of the variable we can use `mutate`:
sp1_depths <- sp1 %>% 
  unite(pretty_depth, top, bottom, sep = '--') %>% 
  mutate(pretty_depth = str_c(pretty_depth, 'cm')) 

head(sp1_depths)
```

`separate` is the complement function to `unite`:

```{r separate}
sp1_depths %>% 
  separate(pretty_depth, c('top', 'bottom'), sep = '-') # Split the variable using '--'

sp1_depths %>% 
  separate(
    pretty_depth, c('top', 'bottom'), sep = '--' 
  ) %>% 
  mutate(
    bottom = str_replace(bottom, 'cm', '') # Remove "cm"
  ) 

sp1_depths %>% 
  separate(
    pretty_depth, c('top', 'bottom'), sep = '--'
  ) %>% 
  mutate(
    bottom = str_replace(bottom, 'cm', ''),
    top = as.numeric(top), # Convert `top` from character to numeric
    bottom = as.numeric(bottom) # Convert `bottom` from character to numeric
  )
```

### Joins

A very useful set of tools in your data wrangling toolbox are the **joins**. 

This operation joins two data sources together. For example you might have additional data that is stored in a different file.

There are several options to do this in R: `merge` is additional in `base` R, `join` from the `plyr` package. I found the join operators from the `dplyr` package to b faster and easier to use.

Let's start with a **left join**: you have a "master" copy of your data (on your left hand), and you will add columns from an additional data source (on your right). What makes it a left join is that only the records from this additional dataset that matches the records present in your "master" dataset will be merged. 


```{r join0}
additional_data <- data.frame(
  id = unique(sp1$id),
  more_data = runif(9),
  even_more_data = LETTERS[1:9]
)
head(additional_data)

left_join(sp1, additional_data)
```

```{r join1}
additional_data <- data.frame(
  id = unique(sp1$id),
  more_data = runif(9),
  even_more_data = LETTERS[1:9]
)
head(additional_data)

left_join(sp1, additional_data, by = 'id')
```

Slighly more complex case: when the variables to join the two `data.frame` are named differently: 
```{r join_2}
additional_data <- data.frame(
  some_id = unique(sp1$id),
  more_data = runif(9),
  even_more_data = LETTERS[1:9]
)
head(additional_data)

left_join(sp1, additional_data, by = c('id' = 'some_id'))
```

```{r join_3}
additional_data <- data.frame(
  some_id = unique(sp1$id),
  some_group = sample(1:2, size = 9, replace = TRUE),
  more_data = runif(9),
  even_more_data = LETTERS[1:9]
)
head(additional_data)

left_join(
  sp1, additional_data, 
  by = c(
    'id' = 'some_id',
    'group' = 'some_group'
  )
)
```

- `inner_join`: return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned.
- `left_join`: return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.
- `right_join`: return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.
- `semi_join`: return all rows from x where there are matching values in y, keeping just columns from x. A semi join differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x.
- `anti_join`: return all rows from x where there are not matching values in y, keeping just columns from x.
- `full_join`: return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing.

### Grouped operations

Some operations will need to be grouped according to a given attribute --- for example per profile, or per region, etc. 

The `group_by` operator modify your `data.frame` so to apply subsequent oerations sequencially on these groups:

```{r group}
sp1 %>% 
  group_by(id) %>% 
  summarise(mean_ph = mean(field_ph, na.rm = TRUE))

sp1 %>% 
  group_by(hue, bound_topography) %>% 
  summarise(
    number_of_profiles = n(),
    mean_ph = mean(field_ph, na.rm = TRUE)
  )
```

### Apply arbitrary functions

When the function you apply to all or a group of rows does **not** return a unique value, but something more complex --- eg `anova` or `lm`.

In this case, `do` allows you to create a specific type of column: the *list-column*.

```{r do}
# Fit a linear model for each level of column "group"
sp1 %>% 
  group_by(group) %>% 
  do(
    model = lm(field_ph ~ hue + value + chroma, data = .)
  )

# Extract R-squared from each fitted model
sp1 %>% 
  group_by(group) %>% 
  do(
    model = lm(field_ph ~ hue + value + chroma, data = .)
  ) %>% 
  summarise(
    rsq = summary(model)$r.squared
  )
```

```{r rf}
library(randomForest)

# A silly example to show that you can store many things --- like a random forest model
sp1 %>% 
  select(group, field_ph, chroma, value) %>% 
  na.omit %>% 
  group_by(group) %>% 
  do(fit = randomForest::randomForest(field_ph ~  value + chroma, data = .))
```

## From wide to long (and back)

There are two ways to store your tidy data:

- **wide**: each row is an observation, each column is a measured variable for that observation 
- **long**: each row is one measurement for a given observation

```{r wide_long, echo=FALSE, warning=FALSE}
sp1 %>% 
  as_tibble

sp1 %>% 
  as_tibble %>% 
  select(group:field_ph) %>% 
  gather(variable, value, -(group:bottom))
```

Each "format" has its own use and it's useful to be able to go from one to another

```{r tidyr}
sp1 %>% 
  select(id, top, bottom, hue, value, chroma) %>% 
  gather(key, val)

sp1 %>% 
  select(id, top, bottom, hue, value, chroma) %>% 
  gather(key, val, -id, -top, -bottom)
```

This sort of formatting into key-value pairs is widely used by some plotting systems such as `ggplot2`. Here's just a taster of what's possible:

```{r ggplot, warning=FALSE}
data(sp4, package = "aqp")

head(sp4)

sp4 <- as_tibble(sp4)

sp4 %>% 
  gather(key, val, -(id:bottom)) %>% 
  ggplot() +
    geom_boxplot(aes(x = name, y = val)) + 
    facet_wrap(~key, scales = "free") 
```

```{r tidyr_2}
wide <- sp4 %>% 
  gather(variable, value, -(id:bottom))

head(wide)

long <- wide %>% 
  spread(variable, value)

head(long)
```

# Bonus: handling conflicts between packages

Some packages will use the same name for different functions. 

This is handle by the `NAMESPACE` -- however it is pretty basic, and long story short the last loaded package is "masking" the other functions with the same name.

A simple and efficient workaround this is to use the `::` operator, that allow to by-pass the packages' `NAMESPACE`.

You simply specify the package name then the function name. This way it is clear which package you are using: `package::function(...)`

```{r conflicts, warning=FALSE}
library(plyr)

# In this case the grouping does not work
sp4 %>% 
  group_by(id) %>% 
  summarise(
    mean_K = mean(K),
    mean_Mg = mean(Mg)
  )

# .... that's because our pipeline was using plyr::summarise 
# instead of dplyr::summarise!
sp4 %>% 
  group_by(id) %>% 
  dplyr::summarise(
    mean_K = mean(K),
    mean_Mg = mean(Mg)
  )
```